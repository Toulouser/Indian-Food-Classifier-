{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with MobileNetV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from train_utils.augmenters import data_augmenter, data_augmenter_fruit360\n",
    "from train_utils.callbacks import LossHistory, LRCallBack\n",
    "from train_utils.utils import is_in\n",
    "from train_utils.losses import crossentropy_loss\n",
    "from train_utils.dataset import food_dataset\n",
    "from train_utils.lite_accuracy import evaluate_model\n",
    "from train_utils.metadata_writer_for_image_classifier import generate_metadata\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 2 - Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 195823 files belonging to 296 classes.\n",
      "Using 193865 files for training.\n",
      "Found 195823 files belonging to 296 classes.\n",
      "Using 1958 files for validation.\n",
      "Found 89089 files belonging to 96 classes.\n",
      "tf.Tensor(6059, shape=(), dtype=int64)\n",
      "<BatchDataset shapes: ((None, 224, 224, 3), (None, 296)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "train_dataset, cv_dataset = food_dataset(BATCH_SIZE, IMG_SIZE)\n",
    "\n",
    "from train_utils.dataset import fruit360_classes, all_class_names\n",
    "\n",
    "print(\n",
    "    tf.data.experimental.cardinality(train_dataset),\n",
    "    train_dataset,\n",
    "    sep = '\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(all_class_names[tf.math.argmax(labels[i])])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 3 - Preprocess and Augment Training Data\n",
    "(https://www.tensorflow.org/tutorials/images/data_augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=128, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_augmentation = data_augmenter_fruit360()\n",
    "\n",
    "for image, _ in train_dataset.take(1):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    first_image = image[1]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "        plt.imshow(augmented_image[0] / 255)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 4 - Using MobileNetV3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "mobile_v3_model = tf.keras.applications.MobileNetV3Large(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the last 2 layers here. They are the so called top layers, and they are responsible of the classification in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  269\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of layers in the base model: \", len(mobile_v3_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3) (32, 296)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "print(image_batch.shape, label_batch.shape)\n",
    "NUM_CLASSES = label_batch.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0 to skip\n",
    "FIXED_LAYERS_V3 = 256    # [0, 276]\n",
    "DROPOUT_BEFORE = 0       # [0, 0.5]\n",
    "TOP_DENSE_LAYER = 0    # (300, 1280) U {0}\n",
    "DROPOUT_AFTER = 0        # [0, 0.5]\n",
    "L1, L2 = 0, 0            # [1e-9, 1e-4] U {0}\n",
    "GRADIENT_CLIP = 100       # [5, 100]\n",
    "\n",
    "LR = (-4, -6)            # [-5.5, -2.5]\n",
    "EPOCHS = 10              # [0, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    food_augmenter = data_augmenter()\n",
    "    fruit_augmenter = data_augmenter_fruit360()\n",
    "    fruit_indices = tf.constant(\n",
    "        [all_class_names.index(fruit) for fruit in fruit360_classes]\n",
    "    )\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) \n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        class_indices = tf.math.argmax(y, -1)\n",
    "        isfr = is_in(class_indices, self.fruit_indices)\n",
    "        isfr = tf.expand_dims(tf.expand_dims(tf.expand_dims(isfr, -1), -1), -1)\n",
    "        x = tf.where(\n",
    "            isfr,\n",
    "            self.fruit_augmenter(x),\n",
    "            self.food_augmenter(x),\n",
    "        )\n",
    "        return super().train_step((x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = IMG_SIZE + (3,)\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV3Large(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    ")\n",
    "base_model.trainable = True\n",
    "fine_tune_at = FIXED_LAYERS_V3\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "def food_model():\n",
    "    reg = tf.keras.regularizers.L1L2(L1, L2)\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = base_model(inputs)\n",
    "    x = tfl.GlobalAveragePooling2D()(x)\n",
    "    if DROPOUT_BEFORE:\n",
    "        x = tf.keras.layers.Dropout(DROPOUT_BEFORE)(x)\n",
    "    if TOP_DENSE_LAYER:\n",
    "        x = tfl.Dense(\n",
    "            TOP_DENSE_LAYER,\n",
    "            activation='relu', \n",
    "            kernel_regularizer=reg)(x)\n",
    "    if DROPOUT_AFTER:\n",
    "        x = tf.keras.layers.Dropout(DROPOUT_AFTER)(x)\n",
    "    outputs = tfl.Dense(\n",
    "        NUM_CLASSES, \n",
    "        activation='softmax', \n",
    "        kernel_regularizer=reg)(x)\n",
    "    \n",
    "    model = CustomModel(\n",
    "        inputs, \n",
    "        outputs\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate optimal learning rate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "history = LossHistory(batches=200, l_r=(-5, -0.5))\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=GRADIENT_CLIP)\n",
    "\n",
    "model2 = food_model()\n",
    "model2.compile(\n",
    "    loss=crossentropy_loss,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "model2.fit(\n",
    "    train_dataset.take(history.batches),\n",
    "    callbacks=[history],\n",
    ")\n",
    "    \n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(history.exp_lrs, history.losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Learning Rate Exponent')\n",
    "plt.title('Training Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "MobilenetV3large (Functional (None, 1, 1, 1280)        4226432   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_14  (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 296)               379176    \n",
      "=================================================================\n",
      "Total params: 4,605,608\n",
      "Trainable params: 1,764,776\n",
      "Non-trainable params: 2,840,832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(clipvalue=GRADIENT_CLIP)\n",
    "\n",
    "metrics = [\n",
    "    tf.keras.metrics.CategoricalAccuracy(name='top1'),\n",
    "    tf.keras.metrics.TopKCategoricalAccuracy(k=4, name='top4'),\n",
    "]\n",
    "model2 = food_model()\n",
    "model2.compile(\n",
    "    loss=crossentropy_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=metrics,\n",
    ")\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tqdm_callback = tfa.callbacks.TQDMProgressBar(\n",
    "    metrics_separator=' ; ',\n",
    "    epoch_bar_format='{n_fmt}/{total_fmt} ; ETA: {remaining}s {bar} {desc}',\n",
    "    metrics_format='{name}: {value:0.4f}',\n",
    "    update_per_second=1,\n",
    ")\n",
    "lr_callback = LRCallBack(epochs=EPOCHS, l_r=LR)\n",
    "\n",
    "history = model2.fit(\n",
    "    train_dataset,\n",
    "    validation_data=cv_dataset,\n",
    "    epochs=lr_callback.epochs,\n",
    "    verbose=0,\n",
    "    callbacks=[tqdm_callback, lr_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.plot(lr_callback.batch_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Batches')\n",
    "plt.title('Training Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc = history.history['top1']\n",
    "val_acc = history.history['val_top1']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,0.01])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('models\\\\main.h5')\n",
    "model2.save_weights('models\\\\main_weights.h5')\n",
    "with open(\"models\\\\main_classes.txt\", 'w') as file:\n",
    "    for class_name in all_class_names:\n",
    "        file.write(class_name + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune Quantization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model2.load_weights('models\\\\mobilenet_v3_large_food_classifier_weights.h5')\n",
    "model2.evaluate(cv_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model2)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Quant TFLite test_accuracy:', test_accuracy)\n",
    "print('Quant TF test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"models\\\\mobilenet_v3_large_food_classifier_op.tflite\", \"wb\").write(tflite_model)\n",
    "generate_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Network Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(cv_dataset))\n",
    "plt.figure(figsize=(25, 25))\n",
    "for item in range(9):\n",
    "    ax = plt.subplot(3, 3, item + 1)\n",
    "    plt.imshow(image_batch[item].numpy().astype(\"uint8\"))\n",
    "    probs = model2(image_batch)[item]\n",
    "    label = all_class_names[tf.math.argmax(label_batch[item])]\n",
    "    pred = all_class_names[tf.math.argmax(probs)]\n",
    "    plt.title(label + '-' + pred)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    'datasets\\\\test',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    label_mode='categorical',\n",
    ")\n",
    "test_labels = test_dataset.class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(test_dataset))\n",
    "plt.figure(figsize=(25, 25))\n",
    "for item in range(1):\n",
    "    ax = plt.subplot(3, 3, item + 1)\n",
    "    plt.imshow(image_batch[item].numpy().astype(\"uint8\"))\n",
    "    probs = model2(image_batch)[item]\n",
    "    print(image_batch[item])\n",
    "    label = test_labels[tf.math.argmax(label_batch[item])]\n",
    "    pred = all_class_names[tf.math.argmax(probs)]\n",
    "    plt.title(label + '-' + pred)\n",
    "    plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
